I am using ml-agents for training a robot arm do inverse kinematics because its structure is super complex and multi-axes so normal build in kinematics modules in unity probably wont work. The output of the neural network comes down to 1 axes rotation for each module (right now etc 6). Initially I was training it on episodes of something like 1000 steps to make the end effector reach the target but that did not produces wanted results - I am doing this so I can replicate it in real life with stepper motors and 3d printed parts (my hardware team). Wondered if ml-agents would be suited for a singe step episodes or more like single input single output type of learning because that will be more ideal for my robot arm when it comes to implementing it in real life (trust). Can you write me an example ArmAgent script and a arm_config.yaml file for that approach? I think that when it comes to observations the arm would only need something like the 3d position of the target relative to its base - I have no problem with training a new model/brain if I change something in the arm physically (etc if i change its lenght i will train a new model because i have blazingly fast cuda + 3060 gpu). If you think of something more that the arm would need besides 3 inputs (axes of the target possition) and 6 outputs (1 per part) tell me but for just a initial test of the approach it should probably be fine and only when that works we will implement more observation and actions. You can leave it to me to adjust the traning with stuff like randomising the target poss and stuff I only need help for the hard part - the nn code and config. 
